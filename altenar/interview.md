# Список вопросов:

1. Как шедулировать поды только на определенном ноде пулле? (k8s)
2. Что такое Pre-installations в Kubernetes? (k8s)
3. Как жестко прикрепить под к ноде? Можно ли сделать это через NodeSelector? (k8s)
4. Какие способы есть чтобы приатачить волюмы к поду / деплойменту? (k8s)
5. Какая разница между PersistentVolume (PV) и PersistentVolumeClaim (PVC)? (k8s)
6. Можем ли использовать дисковое пространство ноды, на которой у нас находится под? Как это сделать? (k8s)
7. Как k8s управляет сетью в кластере? Какие основные объекты использует? (k8s)
8. Какие типы сервисов знаешь в k8s? (k8s)
9. Как работает тип сервиса NodePort? (k8s)
10. Есть необходимость ограничивать трафик внутри кубера и даже наружу. Каким образом это можно сделать? (k8s)
11. Нужно настроить регулярное резервное копирование данных etcd каждый день в 00-00. Как сделать это? (k8s)
12. Мы хотим настроить регулярную дефрагментацию etcd каждый день в 00-00. Как нам сделать это? (k8s)
13. Что такое дефрагментация etcd и зачем она нужна? (k8s)
14. Нужно создать директорию на Linux сервере и сделать так чтобы права внутри этой директории наследовались также для всех правил. Каким образом можно это сделать? (linux)
15. Что такое основные и дополнительные биты в Unix/Linux системах? (linux)
16. 

## Собеседование с Altenar:

-----------------------

# Вопросы из записи в iCloud (Тех собес Altenar 1/1):
Ссылка на - Troubleshooting Scenarios (задания для DevOps Собеседований) https://sadservers.com/scenarios


## 1. Нужно развернуть инстансы MongoDB в Google Cluster. У нас есть 2 НП. 1 преэмптив пулл и 2ой кор пулл. Задача: поды MongoDB шедулились только на определенном нод пулле (кор пулл). Как технически решить эту задачу? (k8s)

Для того чтобы развернуть инстансы MongoDB в Google Kubernetes Engine (GKE) и гарантировать, что они будут шедулироваться только на определенном ноде пуле (кор пулл), можно воспользоваться следующими шагами:

1. Создайте два Node Pool в вашем кластере GKE: один для преэмптивных экземпляров (преэмптив пулл) и другой для обычных экземпляров (кор пулл).

2. Настройте аннотации для Node Pool, чтобы указать, на каких узлах должны запускаться поды MongoDB. Например, вы можете добавить аннотацию к узлам в кор пулле:

```yaml
cloud.google.com/gke-nodepool: "core-pool"
```

3. В манифесте для развертывания MongoDB в кластере GKE, укажите селектор, чтобы поды MongoDB были шедулированы только на узлы с определенной аннотацией. Например:

```yaml
nodeSelector:
  cloud.google.com/gke-nodepool: core-pool
```

4. Примените манифест для развертывания MongoDB в вашем кластере GKE. Теперь поды MongoDB будут шедулироваться только на узлы в кор пулле, так как они соответствуют селектору.

Эти шаги позволят вам контролировать, на каких узлах вашего кластера GKE будут запускаться поды MongoDB, обеспечивая таким образом желаемое размещение и управление инстансами MongoDB.

1) **Node Pool**: В Google Kubernetes Engine (GKE) Node Pool - это группа узлов (нод), которая представляет собой подмножество узлов в вашем кластере GKE. Каждый Node Pool может иметь свои собственные характеристики, такие как тип машины, размер, автоскалирование и т. д.

2) **Преэмптивный пулл (Preemptible Pool)**: Преэмптивные экземпляры Google Cloud - это дешевые виртуальные машины, которые могут быть прерваны в любой момент Google. Преэмптивный пулл в GKE - это Node Pool, состоящий из преэмптивных экземпляров, которые могут быть выгодны для выполнения ненадежных задач или задач, которые могут быть перезапущены без потери данных.

3) **Кор пулл (Core Pool)**: Кор пулл в контексте GKE обычно означает основной пул узлов, который содержит обычные (не преэмптивные) экземпляры виртуальных машин.

4) Для шедулирования подов на конкретную ноду на физических серверах (нодах), если у вас нет прав на просмотр списка всех нод, можно воспользоваться аннотациями к нодам. В манифесте пода можно указать аннотацию, которая соответствует конкретной ноде, на которой вы хотите запустить под.

5) Пример с помощью Helm для создания пода на конкретной ноде с определенной аннотацией:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    node.alpha.kubernetes.io/ttl: "0"  # Аннотация для выбора ноды
spec:
  containers:
    - name: my-container
      image: nginx:latest
```

В этом примере, мы указали аннотацию `node.alpha.kubernetes.io/ttl: "0"`, которая поможет шедулировать под на конкретную ноду. После установки Helm чарта с таким манифестом, под будет запущен на ноде, соответствующей указанной аннотации.


## 2. Что такое Pre-installations в Kubernetes? (k8s)

Прединсталляции (Pre-installations) в Kubernetes - это процесс предварительной настройки и установки необходимых компонентов или ресурсов, которые будут использоваться при дальнейшей установке и развертывании приложений или сервисов. 

Прединсталляции могут включать в себя настройку Persistent Volumes, создание ConfigMaps, установку Helm чартов, настройку сетевых политик и другие операции, которые должны быть выполнены до запуска основных приложений.

Для решения задачи шедулирования подов на конкретную ноду с использованием прединсталляций, можно предварительно настроить аннотации на нодах, а затем при установке приложения или Helm чарта, указать соответствующие аннотации для шедулирования подов на нужные ноды.

Таким образом, прединсталляции могут быть использованы для предварительной настройки окружения Kubernetes, включая шедулирование подов на конкретные ноды, что позволяет более гибко управлять развертыванием приложений в кластере.

Для решения задачи шедулирования подов на конкретную ноду с использованием прединсталляций в Kubernetes, можно использовать следующий подход:

1. Создайте прединсталляцию, которая устанавливает аннотации на нодах, на которых вы хотите шедулировать поды. Например, вы можете создать ConfigMap с аннотациями для нужных нод:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-annotations
data:
  node-1: "node.alpha.kubernetes.io/ttl=0"
  node-2: "node.alpha.kubernetes.io/ttl=0"
```

2. Создайте прединсталляцию, которая применяет ConfigMap с аннотациями к нодам. Например, вы можете создать Job, который применяет аннотации к нодам:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: apply-node-annotations
spec:
  template:
    spec:
      containers:
      - name: apply-node-annotations
        image: kubectl
        command: ["kubectl", "annotate", "node", "node-1", "node-2", "-f", "/path/to/node-annotations.yaml"]
      restartPolicy: Never
```

3. После выполнения прединсталляции, при установке приложения или Helm чарта, укажите соответствующие аннотации для шедулирования подов на нужные ноды. Например, в манифесте пода:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    node.alpha.kubernetes.io/ttl: "0"  # Аннотация для выбора ноды
spec:
  containers:
    - name: my-container
      image: nginx:latest
```

Таким образом, вы можете использовать прединсталляции в Kubernetes для предварительной настройки аннотаций на нодах и шедулирования подов на конкретные ноды в кластере.

## 3. Как жестко прикрепить под к ноде? Можно ли сделать это через NodeSelector? (k8s)

Можно жестко привязать под к определенной ноде в Kubernetes с помощью NodeSelector. NodeSelector - это механизм, позволяющий выбирать ноду на основе меток (labels), присвоенных ноде. При создании пода, можно указать NodeSelector, чтобы указать на какой ноде должен быть развернут этот под.

Вот пример как можно жестко привязать под к ноде с помощью NodeSelector:

1. Сначала необходимо присвоить метку выбранной ноде. Для этого можно воспользоваться командой:

```bash
kubectl label nodes <node-name> my-node=selected
```

2. Затем, в манифесте пода укажите NodeSelector, чтобы привязать под к ноде с меткой "my-node=selected":

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
  nodeSelector:
    my-node: selected
```

При создании этого пода, Kubernetes будет пытаться разместить его на ноде, которая имеет метку "my-node=selected". Если на ноде с такой меткой нет свободных ресурсов, под будет ожидать, пока такая нода не станет доступной.

## 4. Какие способы есть чтобы приатачить волюмы к поду / деплойменту?Приведи несколько примеров. (k8s)

В Kubernetes существует несколько способов прикрепления томов (волюмов) к подам или деплойментам:

1. EmptyDir:
EmptyDir - это временное хранилище, связанное с жизненным циклом пода. EmptyDir создается при запуске пода и удаляется при его остановке. Пример манифеста пода с использованием EmptyDir:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      emptyDir: {}
```

2. PersistentVolume (PV) и PersistentVolumeClaim (PVC):
PV и PVC используются для создания постоянных хранилищ в Kubernetes. PV представляет собой независимый объем данных, а PVC запрашивает хранилище и связывается с PV. Пример PVC и манифеста пода с использованием PVC:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc
```

3. HostPath:
HostPath позволяет монтировать локальный путь на хосте в под. Этот метод не рекомендуется для продакшн сред, так как он приводит к привязке пода к конкретному узлу. Пример манифеста пода с использованием HostPath:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      hostPath:
        path: /path/on/host
```

Это несколько способов прикрепления томов к подам или деплойментам в Kubernetes. Каждый из них имеет свои особенности и подходит для различных сценариев использования.

## 5. Какая разница между PersistentVolume (PV) и PersistentVolumeClaim (PVC)? (k8s)

PersistentVolume (PV) и PersistentVolumeClaim (PVC) - это два ключевых объекта в Kubernetes, которые позволяют управлять постоянными хранилищами в кластере. Вот основные различия между ними:

1. PersistentVolume (PV):
- PV - это абстракция, представляющая собой независимый объем данных в кластере Kubernetes.
- PV создается администратором кластера и предоставляет доступ к физическим хранилищам, таким как NFS, iSCSI, AWS EBS и т.д.
- PV можно использовать для централизованного управления хранилищами и предоставления доступа к ним различным подам.
- PV имеет различные атрибуты, такие как емкость, тип хранилища, доступные режимы доступа и т.д.

2. PersistentVolumeClaim (PVC):
- PVC - это запрос на хранилище, который создается пользователем при создании пода или другого ресурса.
- PVC запрашивает определенный объем данных и определяет требования к хранилищу, такие как емкость и режим доступа.
- PVC связывается с PV, который соответствует его требованиям, и Kubernetes автоматически связывает PVC с PV.
- PVC позволяет пользователям работать с хранилищем без необходимости знать о конкретных деталях физического хранилища.

Таким образом, PV представляет собой реальное хранилище, которое предоставляется администратором, а PVC - это запрос на использование этого хранилища со стороны пользователя. PV и PVC работают вместе для обеспечения динамического и управляемого доступа к постоянным хранилищам в Kubernetes.

## 6. Можем ли использовать дисковое пространство ноды, на которой у нас находится под? Как это сделать? (k8s)

В Kubernetes есть возможность использовать дисковое пространство на узле (Node) с помощью механизма `hostPath`. `hostPath` позволяет монтировать локальный путь на хосте (узле) в под. Этот метод может быть полезен в некоторых случаях, но следует использовать его с осторожностью, так как он связывает под с конкретным узлом и не обеспечивает масштабируемость и надежность, характерные для облачных хранилищ.

Вот пример манифеста пода, который использует `hostPath` для доступа к дисковому пространству на узле:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      hostPath:
        path: /path/on/host
```

В этом примере под `my-pod` будет монтировать путь `/path/on/host` с узла, на котором он запущен, внутрь контейнера по пути `/data`. Подобный подход может быть полезен для доступа к локальным файлам или ресурсам, которые находятся на узле.

Однако следует помнить, что использование `hostPath` может привести к проблемам с масштабируемостью, безопасностью и управляемостью, поэтому рекомендуется использовать более надежные и гибкие решения хранения данных, такие как PersistentVolumes и PersistentVolumeClaims, особенно в продакшн средах.

## 7. Как k8s управляет сетью в кластере? Какие основные объекты использует? (k8s)

В Kubernetes (K8s) управление сетью в кластере осуществляется с помощью основных объектов, таких как:

1. **Pods (Поды)**: Поды - это минимальные запускаемые единицы в Kubernetes. Они могут включать в себя один или несколько контейнеров. Каждый под имеет свой уникальный IP-адрес внутри кластера.

2. **Services (Сервисы)**: Сервисы предоставляют абстракцию для доступа к группе подов. Они могут быть использованы для балансировки нагрузки, обеспечения доступности и масштабируемости приложений.

3. **Ingress (Вход)**: Ingress - это ресурс, который управляет внешним доступом к службам в кластере Kubernetes. Он позволяет настраивать маршрутизацию HTTP и HTTPS трафика на основе правил.

4. **Network Policies (Политики сети)**: Политики сети позволяют управлять трафиком между подами и сервисами в кластере. Они могут определять правила доступа на основе IP-адресов, портов и других параметров.

## 8. Какие типы сервисов знаешь в k8s? (k8s)

В Kubernetes (k8s) существует несколько типов сервисов, которые позволяют управлять доступом к подам в кластере. Вот некоторые из них:

1. **ClusterIP**: Этот тип сервиса назначает внутренний IP-адрес сервису, который доступен только внутри кластера. Это означает, что сервис доступен только другим ресурсам внутри кластера.

2. **NodePort**: Сервис типа NodePort открывает определенный порт на каждом узле (ноде) кластера и перенаправляет трафик на соответствующий сервис. Это позволяет внешним клиентам обращаться к сервису через адрес узла и порт.

3. **LoadBalancer**: Тип сервиса LoadBalancer позволяет использовать внешний балансировщик нагрузки, который будет распределять трафик между подами сервиса. Этот тип сервиса доступен только в некоторых облачных провайдерах.

4. **ExternalName**: Сервис типа ExternalName позволяет предоставить внешнее DNS-имя для доступа к сервису. Поды в кластере будут использовать внешний DNS-сервер для обращения к этому имени.

Эти типы сервисов позволяют гибко управлять доступом к подам в Kubernetes в зависимости от требований вашего приложения.

## 9. Как работает тип сервиса NodePort? (k8s)

Тип сервиса NodePort в Kubernetes открывает определенный порт на каждом узле (ноде) в кластере и перенаправляет трафик на соответствующий сервис. Это позволяет внешним клиентам обращаться к сервису через адрес узла и порт.

Вот как это работает:

1. Когда вы создаете сервис с типом NodePort, Kubernetes назначает случайный порт из диапазона 30000-32767 на каждом узле в кластере.

2. Когда клиент обращается к сервису через порт NodePort, трафик направляется на этот порт на каждом узле.

3. После получения запроса на порт NodePort, kube-proxy, компонент Kubernetes, который отвечает за маршрутизацию трафика внутри кластера, перенаправляет трафик на соответствующий сервис и под внутри кластера.

4. Поды, связанные с этим сервисом, могут обрабатывать запросы и возвращать ответы через NodePort.

Таким образом, тип сервиса NodePort позволяет обеспечить доступ к сервису извне кластера, используя адрес узла и порт, назначенный сервису.

## 10. Есть необходимость ограничивать трафик внутри кубера и даже наружу. Каким образом это можно сделать? (k8s)

Для ограничения трафика внутри кластера Kubernetes и наружу можно использовать различные методы и инструменты. Вот несколько вариантов:

1. **Network Policies**: С помощью объектов NetworkPolicy можно определить правила доступа к подам в кластере на основе IP-адресов, портов, меток и других параметров. Это позволяет ограничить трафик между подами внутри кластера и наружу. Пример создания NetworkPolicy:

   ```yaml
   apiVersion: networking.k8s.io/v1
   kind: NetworkPolicy
   metadata:
     name: allow-nginx
   spec:
     podSelector:
       matchLabels:
         app: nginx
     policyTypes:
     - Ingress
     - Egress
     ingress:
     - from:
       - ipBlock:
           cidr: 192.168.0.0/16
       ports:
       - protocol: TCP
         port: 80
     egress:
     - to:
       - ipBlock:
           cidr: 10.0.0.0/8
       ports:
       - protocol: TCP
         port: 443
   ```

2. **Сетевые решения от облачных провайдеров**: Некоторые облачные провайдеры предоставляют собственные сетевые решения для управления трафиком в кластере. Например, AWS предлагает Security Groups и Network ACLs для ограничения доступа к ресурсам в облаке.

3. **Использование сервисов LoadBalancer**: Если ваш кластер поддерживает тип сервиса LoadBalancer, вы можете использовать внешний балансировщик нагрузки для фильтрации трафика и ограничения доступа к сервисам.

4. **Использование сетевых решений от сторонних поставщиков**: Некоторые компании предлагают решения для управления сетевым трафиком в Kubernetes, такие как Calico, Cilium, Weave Net и другие.

Для настройки и управления сетевыми политиками в Kubernetes можно использовать утилиту `kubectl` для создания и применения объектов NetworkPolicy. Например, чтобы применить файл с описанием NetworkPolicy, можно использовать команду:

```bash
kubectl apply -f networkpolicy.yaml
```

## 11. Нужно настроить регулярное резервное копирование данных etcd каждый день в 00-00. Как сделать это? (k8s)

Для регулярной резервной копии данных etcd в кластере Kubernetes можно использовать утилиту `etcdctl`, которая позволяет взаимодействовать с etcd. Ниже приведены шаги и пример того, как настроить регулярную инфрагментацию etcd каждый день в 00:00:

1. **Создание скрипта для резервного копирования**:
   Создайте скрипт резервного копирования, который будет выполнять резервное копирование etcd данных. Вот пример скрипта `backup_etcd.sh`:

   ```bash
   #!/bin/bash
   ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 \
   --cacert=/etc/kubernetes/pki/etcd/ca.crt \
   --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt \
   --key=/etc/kubernetes/pki/etcd/healthcheck-client.key \
   snapshot save /path/to/etcd-backup-$(date "+%Y-%m-%d").db
   ```

2. **Настройка задачи Cron**:
   Для выполнения резервного копирования каждый день в 00:00 на всех нодах кластера настройте задачу Cron. Добавьте следующую запись в файл крон-задач `/etc/crontab` на каждой ноде:

   ```bash
   0 0 * * * root /path/to/backup_etcd.sh
   ```

3. **Убедитесь, что скрипт имеет правильные разрешения**:
   Убедитесь, что скрипт `backup_etcd.sh` имеет правильные разрешения для выполнения. Для этого выполните команду:

   ```bash
   chmod +x /path/to/backup_etcd.sh
   ```

4. **Проверка резервного копирования**:
   Для проверки резервного копирования etcd данных выполните скрипт вручную:

   ```bash
   /path/to/backup_etcd.sh
   ```

Теперь, каждый день в 00:00, скрипт будет выполняться автоматически на каждой ноде кластера и создавать резервную копию данных etcd. Важно убедиться, что резервные копии сохраняются в безопасном месте и доступны для восстановления в случае необходимости.

## 12. Мы хотим настроить регулярную дефрагментацию etcd каждый день в 00-00. Как нам сделать это? (k8s)

Для настройки регулярной дефрагментации etcd каждый день в 00:00 на всех нодах вашего кубернетес кластера, вы можете воспользоваться задачами Cron. Вот пример шагов и инструкций, как это сделать:

1. **Создание скрипта для дефрагментации**:
   Создайте скрипт, который будет выполнять дефрагментацию etcd. Например, создайте файл `defrag_etcd.sh` и добавьте в него следующий код:

   ```bash
   #!/bin/bash
   ETCDCTL_API=3 etcdctl defrag
   ```

2. **Добавление правильных разрешений**:
   Убедитесь, что скрипт `defrag_etcd.sh` имеет правильные разрешения для выполнения. Выполните следующую команду на каждой ноде кластера:

   ```bash
   chmod +x /path/to/defrag_etcd.sh
   ```

3. **Настройка задачи Cron**:
   Добавьте задачу Cron для выполнения скрипта дефрагментации каждый день в 00:00. Откройте файл крон-задач `/etc/crontab` на каждой ноде и добавьте следующую запись:

   ```bash
   0 0 * * * root /path/to/defrag_etcd.sh
   ```

4. **Проверка настройки**:
   После настройки задачи Cron, убедитесь, что она правильно запускается. Для этого вы можете подождать до 00:00 или выполнить команду вручную для немедленного запуска:

   ```bash
   /path/to/defrag_etcd.sh
   ```

Теперь, каждый день в 00:00, задача Cron будет автоматически запускать скрипт дефрагментации etcd на всех нодах вашего кубернетес кластера. Это поможет оптимизировать хранилище данных etcd и поддерживать его производительность.

## 13. Что такое дефрагментация etcd и зачем она нужна? (k8s)

Дефрагментация etcd - это процесс оптимизации хранилища данных etcd путем удаления устаревших, удаленных или неиспользуемых данных из журнала транзакций etcd. Этот процесс позволяет уменьшить объем данных хранилища etcd и повысить его производительность.

Вот несколько причин, по которым дефрагментация etcd может быть полезной:

1. **Оптимизация производительности**: После длительного времени использования etcd может накапливаться большое количество устаревших данных, что может замедлить операции чтения и записи. Дефрагментация помогает уменьшить объем данных и улучшить производительность.

2. **Экономия места**: Устаревшие и неиспользуемые данные занимают место в хранилище etcd. Дефрагментация позволяет освободить это место и уменьшить общий объем хранилища.

3. **Предотвращение проблем с производительностью**: Неконтролируемое увеличение размера хранилища etcd может привести к проблемам с производительностью и недостаточной доступности данных. Дефрагментация помогает предотвратить такие проблемы.

Для выполнения дефрагментации etcd можно использовать утилиту `etcdctl`. Например, команда для дефрагментации etcd выглядит примерно так:

```bash
ETCDCTL_API=3 etcdctl defrag
```

Важно помнить, что дефрагментация etcd может занимать определенное время и потреблять ресурсы, поэтому рекомендуется выполнять ее в периоды минимальной активности кластера.

## 14. Нужно создать директорию на Linux сервере и сделать так чтобы права внутри этой директории наследовались также для всех правил. Каким образом можно это сделать? (linux)

1. **Установка ACL**:
   Используйте команду `setfacl` для установки прав доступа, которые будут наследоваться для всех файлов и поддиректорий внутри созданной директории. Например, установите права на чтение, запись и выполнение для всех пользователей:

   ```bash
   setfacl -R -m u::rwx,g::rwx,o::rwx /path/to/directory
   ```
   или
   ```bash
   chmod -R u=rwX,g=rwX,o=rwX /path/to/directory
   ```

   В данном примере:
   - `-R` означает рекурсивное применение к поддиректориям и файлам внутри директории.
   - `-m` указывает на установку новых прав доступа.
   - `u::rwx` устанавливает права для владельца (пользователя) на чтение, запись и выполнение.
   - `g::rwx` устанавливает права для группы на чтение, запись и выполнение.
   - `o::rwx` устанавливает права для остальных пользователей на чтение, запись и выполнение.

2. **Проверка настроек**:
   После установки прав доступа, убедитесь, что они применились корректно. Выполните команду `getfacl` для просмотра текущих прав доступа на директорию:

   ```bash
   getfacl /path/to/directory
   ```
   или
   ```bash
   ls -l /path/to/directory
   ```


## 15. Что такое основные и дополнительные биты в Unix/Linux системах? (linux)

Помимо основных битов прав доступа (читать, писать, исполнять) в Unix/Linux системах, существуют еще дополнительные биты, которые могут быть установлены на файлы и директории. Некоторые из этих дополнительных битов включают в себя:

1. **Set User ID (SUID)**:
   - Когда SUID бит установлен на исполняемом файле, файл будет запущен с правами владельца файла, а не с правами пользователя, который запустил файл. Это позволяет программам выполняться с повышенными привилегиями.
   - Установка SUID бита: `chmod u+s filename`
   - Удаление SUID бита: `chmod u-s filename`

2. **Set Group ID (SGID)**:
   - Когда SGID бит установлен на директории, все файлы, созданные в этой директории, наследуют группу этой директории, а не группу пользователя, который создал файл.
   - Установка SGID бита на директорию: `chmod g+s directory`
   - Установка SGID бита на исполняемый файл: `chmod g+s filename`
   - Удаление SGID бита: `chmod g-s directory_or_filename`

3. **Sticky Bit**:
   - Когда Sticky Bit установлен на директории, только владелец файла может удалить файл из этой директории, даже если другие пользователи имеют права записи в эту директорию.
   - Установка Sticky Bit на директорию: `chmod +t directory`
   - Удаление Sticky Bit: `chmod -t directory`

Эти дополнительные биты позволяют управлять поведением файлов и директорий в системе, предоставляя дополнительные возможности и ограничения. Когда устанавливаете эти биты, будьте внимательны и убедитесь, что понимаете их влияние на безопасность и функциональность вашей системы.

## 16. Задание на Docker. (Life coding)
https://sadservers.com/scenario/salta
### Задание:
- Запустить контейнер, который не запускается!

### Для решения задачи выполнил следующие действия:

1. Переключились в режим суперпользователя (root).
2. Проверили наличие образов Docker с помощью команды `docker images`.
3. Перешли в каталог `/home/admin/app/`, где находятся файлы вашего приложения.
4. Создали Dockerfile для вашего приложения.
5. Собрали Docker образ с помощью команды `docker build -t app:0.0.1 .`.
6. Попытались запустить контейнер на порту 8888, но получили ошибку "address already in use".
7. Нашли процессы, занимающие порт 8888, с помощью команд `ss -tulpn | grep ':8888'` и `ps aux | grep nginx`.
8. Завершили процессы, занимающие порт 8888, с помощью команд `kill -9 PID`.
9. Повторно запустили контейнер на порту 8888 и успешно получили ответ "Hello World!" при помощи команды `curl localhost:8888`.

Для фикса дефекта на сервере вы сначала завершили процессы, занимающие порт 8888, с помощью команд `kill -9 PID`. Затем вы повторно запустили Docker контейнер на порту 8888 с помощью команды `docker run -d -p 8888:8888 app:0.0.1`.

Таким образом, вы освободили порт 8888, чтобы он мог быть использован вашим Docker контейнером, и успешно получили ответ от вашего веб-приложения при помощи команды `curl localhost:8888`.
