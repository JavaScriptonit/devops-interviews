# Список вопросов:

1. Как шедулировать поды только на определенном ноде пулле? (k8s)
2. Что такое Pre-installations в Kubernetes? (k8s)
3. Как жестко прикрепить под к ноде? Можно ли сделать это через NodeSelector? (k8s)
4. Какие способы есть чтобы приатачить волюмы к поду / деплойменту? (k8s)
5. 

## Собеседование с Altenar:

-----------------------

# Вопросы из записи в iCloud (Тех собес Altenar 1/1):
Ссылка на - Troubleshooting Scenarios (задания для DevOps Собеседований) https://sadservers.com/scenarios

# ОПЫТ:

## 1. Нужно развернуть инстансы MongoDB в Google Cluster. У нас есть 2 НП. 1 преэмптив пулл и 2ой кор пулл. Задача: поды MongoDB шедулились только на определенном нод пулле (кор пулл). Как технически решить эту задачу? (k8s)

Для того чтобы развернуть инстансы MongoDB в Google Kubernetes Engine (GKE) и гарантировать, что они будут шедулироваться только на определенном ноде пуле (кор пулл), можно воспользоваться следующими шагами:

1. Создайте два Node Pool в вашем кластере GKE: один для преэмптивных экземпляров (преэмптив пулл) и другой для обычных экземпляров (кор пулл).

2. Настройте аннотации для Node Pool, чтобы указать, на каких узлах должны запускаться поды MongoDB. Например, вы можете добавить аннотацию к узлам в кор пулле:

```yaml
cloud.google.com/gke-nodepool: "core-pool"
```

3. В манифесте для развертывания MongoDB в кластере GKE, укажите селектор, чтобы поды MongoDB были шедулированы только на узлы с определенной аннотацией. Например:

```yaml
nodeSelector:
  cloud.google.com/gke-nodepool: core-pool
```

4. Примените манифест для развертывания MongoDB в вашем кластере GKE. Теперь поды MongoDB будут шедулироваться только на узлы в кор пулле, так как они соответствуют селектору.

Эти шаги позволят вам контролировать, на каких узлах вашего кластера GKE будут запускаться поды MongoDB, обеспечивая таким образом желаемое размещение и управление инстансами MongoDB.

1) **Node Pool**: В Google Kubernetes Engine (GKE) Node Pool - это группа узлов (нод), которая представляет собой подмножество узлов в вашем кластере GKE. Каждый Node Pool может иметь свои собственные характеристики, такие как тип машины, размер, автоскалирование и т. д.

2) **Преэмптивный пулл (Preemptible Pool)**: Преэмптивные экземпляры Google Cloud - это дешевые виртуальные машины, которые могут быть прерваны в любой момент Google. Преэмптивный пулл в GKE - это Node Pool, состоящий из преэмптивных экземпляров, которые могут быть выгодны для выполнения ненадежных задач или задач, которые могут быть перезапущены без потери данных.

3) **Кор пулл (Core Pool)**: Кор пулл в контексте GKE обычно означает основной пул узлов, который содержит обычные (не преэмптивные) экземпляры виртуальных машин.

4) Для шедулирования подов на конкретную ноду на физических серверах (нодах), если у вас нет прав на просмотр списка всех нод, можно воспользоваться аннотациями к нодам. В манифесте пода можно указать аннотацию, которая соответствует конкретной ноде, на которой вы хотите запустить под.

5) Пример с помощью Helm для создания пода на конкретной ноде с определенной аннотацией:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    node.alpha.kubernetes.io/ttl: "0"  # Аннотация для выбора ноды
spec:
  containers:
    - name: my-container
      image: nginx:latest
```

В этом примере, мы указали аннотацию `node.alpha.kubernetes.io/ttl: "0"`, которая поможет шедулировать под на конкретную ноду. После установки Helm чарта с таким манифестом, под будет запущен на ноде, соответствующей указанной аннотации.


## 2. Что такое Pre-installations в Kubernetes? (k8s)

Прединсталляции (Pre-installations) в Kubernetes - это процесс предварительной настройки и установки необходимых компонентов или ресурсов, которые будут использоваться при дальнейшей установке и развертывании приложений или сервисов. 

Прединсталляции могут включать в себя настройку Persistent Volumes, создание ConfigMaps, установку Helm чартов, настройку сетевых политик и другие операции, которые должны быть выполнены до запуска основных приложений.

Для решения задачи шедулирования подов на конкретную ноду с использованием прединсталляций, можно предварительно настроить аннотации на нодах, а затем при установке приложения или Helm чарта, указать соответствующие аннотации для шедулирования подов на нужные ноды.

Таким образом, прединсталляции могут быть использованы для предварительной настройки окружения Kubernetes, включая шедулирование подов на конкретные ноды, что позволяет более гибко управлять развертыванием приложений в кластере.

Для решения задачи шедулирования подов на конкретную ноду с использованием прединсталляций в Kubernetes, можно использовать следующий подход:

1. Создайте прединсталляцию, которая устанавливает аннотации на нодах, на которых вы хотите шедулировать поды. Например, вы можете создать ConfigMap с аннотациями для нужных нод:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: node-annotations
data:
  node-1: "node.alpha.kubernetes.io/ttl=0"
  node-2: "node.alpha.kubernetes.io/ttl=0"
```

2. Создайте прединсталляцию, которая применяет ConfigMap с аннотациями к нодам. Например, вы можете создать Job, который применяет аннотации к нодам:

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: apply-node-annotations
spec:
  template:
    spec:
      containers:
      - name: apply-node-annotations
        image: kubectl
        command: ["kubectl", "annotate", "node", "node-1", "node-2", "-f", "/path/to/node-annotations.yaml"]
      restartPolicy: Never
```

3. После выполнения прединсталляции, при установке приложения или Helm чарта, укажите соответствующие аннотации для шедулирования подов на нужные ноды. Например, в манифесте пода:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
  annotations:
    node.alpha.kubernetes.io/ttl: "0"  # Аннотация для выбора ноды
spec:
  containers:
    - name: my-container
      image: nginx:latest
```

Таким образом, вы можете использовать прединсталляции в Kubernetes для предварительной настройки аннотаций на нодах и шедулирования подов на конкретные ноды в кластере.

## 3. Как жестко прикрепить под к ноде? Можно ли сделать это через NodeSelector? (k8s)

Можно жестко привязать под к определенной ноде в Kubernetes с помощью NodeSelector. NodeSelector - это механизм, позволяющий выбирать ноду на основе меток (labels), присвоенных ноде. При создании пода, можно указать NodeSelector, чтобы указать на какой ноде должен быть развернут этот под.

Вот пример как можно жестко привязать под к ноде с помощью NodeSelector:

1. Сначала необходимо присвоить метку выбранной ноде. Для этого можно воспользоваться командой:

```bash
kubectl label nodes <node-name> my-node=selected
```

2. Затем, в манифесте пода укажите NodeSelector, чтобы привязать под к ноде с меткой "my-node=selected":

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
  nodeSelector:
    my-node: selected
```

При создании этого пода, Kubernetes будет пытаться разместить его на ноде, которая имеет метку "my-node=selected". Если на ноде с такой меткой нет свободных ресурсов, под будет ожидать, пока такая нода не станет доступной.

## 4. Какие способы есть чтобы приатачить волюмы к поду / деплойменту?Приведи несколько примеров. (k8s)

В Kubernetes существует несколько способов прикрепления томов (волюмов) к подам или деплойментам:

1. EmptyDir:
EmptyDir - это временное хранилище, связанное с жизненным циклом пода. EmptyDir создается при запуске пода и удаляется при его остановке. Пример манифеста пода с использованием EmptyDir:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      emptyDir: {}
```

2. PersistentVolume (PV) и PersistentVolumeClaim (PVC):
PV и PVC используются для создания постоянных хранилищ в Kubernetes. PV представляет собой независимый объем данных, а PVC запрашивает хранилище и связывается с PV. Пример PVC и манифеста пода с использованием PVC:

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi

apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      persistentVolumeClaim:
        claimName: my-pvc
```

3. HostPath:
HostPath позволяет монтировать локальный путь на хосте в под. Этот метод не рекомендуется для продакшн сред, так как он приводит к привязке пода к конкретному узлу. Пример манифеста пода с использованием HostPath:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: my-pod
spec:
  containers:
    - name: my-container
      image: nginx:latest
      volumeMounts:
        - name: my-volume
          mountPath: /data
  volumes:
    - name: my-volume
      hostPath:
        path: /path/on/host
```

Это несколько способов прикрепления томов к подам или деплойментам в Kubernetes. Каждый из них имеет свои особенности и подходит для различных сценариев использования.

## 5. Какая разница между PersistentVolume (PV) и PersistentVolumeClaim (PVC)? (k8s)

PersistentVolume (PV) и PersistentVolumeClaim (PVC) - это два ключевых объекта в Kubernetes, которые позволяют управлять постоянными хранилищами в кластере. Вот основные различия между ними:

1. PersistentVolume (PV):
- PV - это абстракция, представляющая собой независимый объем данных в кластере Kubernetes.
- PV создается администратором кластера и предоставляет доступ к физическим хранилищам, таким как NFS, iSCSI, AWS EBS и т.д.
- PV можно использовать для централизованного управления хранилищами и предоставления доступа к ним различным подам.
- PV имеет различные атрибуты, такие как емкость, тип хранилища, доступные режимы доступа и т.д.

2. PersistentVolumeClaim (PVC):
- PVC - это запрос на хранилище, который создается пользователем при создании пода или другого ресурса.
- PVC запрашивает определенный объем данных и определяет требования к хранилищу, такие как емкость и режим доступа.
- PVC связывается с PV, который соответствует его требованиям, и Kubernetes автоматически связывает PVC с PV.
- PVC позволяет пользователям работать с хранилищем без необходимости знать о конкретных деталях физического хранилища.

Таким образом, PV представляет собой реальное хранилище, которое предоставляется администратором, а PVC - это запрос на использование этого хранилища со стороны пользователя. PV и PVC работают вместе для обеспечения динамического и управляемого доступа к постоянным хранилищам в Kubernetes.

## 6. 



## Продолжить собес с 03:30 